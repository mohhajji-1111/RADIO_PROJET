================================================================================
                    PROBLÃˆMES ET DÃ‰FIS RENCONTRÃ‰S - PROJET RADIOMICS
================================================================================

ğŸ“‹ Ce document liste tous les problÃ¨mes rencontrÃ©s dans chaque phase du projet
   et les solutions appliquÃ©es.


================================================================================
                        PHASE 2: CONVERSION DICOM â†’ NIFTI
================================================================================

âŒ PROBLÃˆME 1: FICHIERS DICOM DISPERSÃ‰S
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Les fichiers DICOM ne sont pas dans un seul dossier
  - Chaque patient a plusieurs sous-dossiers avec des noms diffÃ©rents
  - Exemple: LUNG1-001/1.3.6.../1.3.6.../CT_*.dcm

Solution appliquÃ©e:
  - Recherche rÃ©cursive avec glob("**/*.dcm")
  - Lecture de tous les fichiers DICOM trouvÃ©s
  - Tri par SliceLocation pour reconstruire l'ordre correct

Code clÃ©:
  ```python
  dicom_files = sorted(patient_dir.rglob("*.dcm"))
  ```

âŒ PROBLÃˆME 2: HOUNSFIELD UNITS MANQUANTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Certains DICOM n'ont pas RescaleSlope/RescaleIntercept
  - Sans Ã§a, impossible de calculer les vraies valeurs HU
  - Les valeurs brutes ne sont pas standardisÃ©es

Solution appliquÃ©e:
  - VÃ©rification de la prÃ©sence des tags (0028,1052) et (0028,1053)
  - Valeurs par dÃ©faut: slope=1, intercept=0 si manquants
  - Log warning pour traÃ§abilitÃ©

Code clÃ©:
  ```python
  slope = float(ds.RescaleSlope) if hasattr(ds, 'RescaleSlope') else 1.0
  intercept = float(ds.RescaleIntercept) if hasattr(ds, 'RescaleIntercept') else 0.0
  hu = pixel_array * slope + intercept
  ```

âŒ PROBLÃˆME 3: ORIENTATION ET SPACING VARIABLES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - ImageOrientationPatient varie entre scanners
  - PixelSpacing diffÃ©rent (ex: [0.97, 0.97] vs [1.17, 1.17])
  - SliceThickness inconsistant

Solution appliquÃ©e:
  - Extraction systÃ©matique des mÃ©tadonnÃ©es de chaque DICOM
  - Sauvegarde dans les headers NIfTI (spacing, origin, direction)
  - PrÃ©servation de l'orientation native

RÃ©sultat:
  âœ… 159 patients convertis avec succÃ¨s
  âš ï¸ MÃ©tadonnÃ©es prÃ©servÃ©es pour alignement futur


================================================================================
                    PHASE 2.5: EXTRACTION MULTI-ORGANES (RTSTRUCT)
================================================================================

âŒ PROBLÃˆME 1: NOMS DE STRUCTURES INCONSISTANTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Chaque clinique nomme diffÃ©remment les organes
  - Exemples trouvÃ©s:
    * GTV: "GTV-1", "GTV-2", "GTV_PRIMARY", "GROSS TUMOR VOLUME"
    * Poumon droit: "LUNG_R", "POUMON D", "RIGHT LUNG", "Poumon Droit"
    * CÅ“ur: "HEART", "COEUR", "Heart", "Cor"
    * Moelle: "SPINALCORD", "MOELLE", "CORD", "Moelle Ã©piniÃ¨re"

Solution appliquÃ©e:
  - Mapping flexible avec plusieurs variantes par organe
  - Normalisation des noms (uppercase, strip, sans accents)
  - Recherche par mots-clÃ©s partiels

Code clÃ©:
  ```python
  GTV_VARIANTS = ["GTV", "GROSS", "TUMOR", "TUMEUR"]
  LUNG_R_VARIANTS = ["LUNG_R", "LUNG R", "RIGHT LUNG", "POUMON D", "POUMON DROIT"]
  ```

Impact:
  âœ… 158/159 patients traitÃ©s (99.4%)
  âŒ 1 patient sans CT correspondant (LUNG1-XXX)

âŒ PROBLÃˆME 2: MULTIPLE GTVs (GTV-1, GTV-2, GTV-3...)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Certains patients ont plusieurs tumeurs
  - GTV-1, GTV-2, GTV-3 segmentÃ©s sÃ©parÃ©ment par le mÃ©decin
  - Besoin de les fusionner en un seul label

Solution appliquÃ©e:
  - DÃ©tection de toutes les structures contenant "GTV"
  - Fusion automatique avec opÃ©ration OR (union des masques)
  - Attribution d'un seul label (label 1) pour toutes les tumeurs

Code clÃ©:
  ```python
  for roi_name in rtstruct.get_roi_names():
      if any(variant in roi_name.upper() for variant in GTV_VARIANTS):
          gtv_mask_slice = rtstruct.get_roi_mask_by_name(roi_name)[slice_idx]
          final_gtv_mask = np.logical_or(final_gtv_mask, gtv_mask_slice)
  ```

RÃ©sultat:
  âœ… Toutes les tumeurs fusionnÃ©es en label 1 (GTV)
  ğŸ“Š 1 Ã  6 GTVs par patient dÃ©tectÃ©s

âŒ PROBLÃˆME 3: PTV (PLANNING TARGET VOLUME) ABSENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - PTV devrait Ãªtre prÃ©sent (volume cible pour radiothÃ©rapie)
  - 0% des patients ont un PTV dans les RTSTRUCT
  - Fichiers RTSTRUCT ne contiennent que les structures anatomiques

Analyse:
  - Dataset probablement exportÃ© sans les volumes de planification
  - PTV souvent calculÃ© Ã  partir du GTV (expansion de 5-10mm)
  - Non critique pour la segmentation anatomique

DÃ©cision:
  âœ… Label 2 (PTV) maintenu dans le systÃ¨me pour compatibilitÃ© future
  âš ï¸ Toujours Ã  0 dans le dataset actuel
  ğŸ’¡ Pourrait Ãªtre gÃ©nÃ©rÃ© synthÃ©tiquement (dilatation du GTV)

âŒ PROBLÃˆME 4: CÅ’UR SEGMENTÃ‰ DANS SEULEMENT 28% DES PATIENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - CÅ“ur prÃ©sent dans 44/158 patients (27.8%)
  - IncohÃ©rent car tous les scans thoraciques incluent le cÅ“ur anatomiquement

Analyse:
  - CÅ“ur segmentÃ© uniquement si proche de la tumeur (radiothÃ©rapie)
  - Pour tumeurs Ã©loignÃ©es, pas besoin de dÃ©linÃ©er le cÅ“ur (pas d'organe Ã  risque)
  - Segmentation manuelle = temps mÃ©dical coÃ»teux

Impact sur le modÃ¨le:
  âš ï¸ CÅ“ur sera plus difficile Ã  apprendre (moins d'exemples)
  âš ï¸ Class imbalance sÃ©vÃ¨re (72% background pour le cÅ“ur)
  ğŸ’¡ Solution: Class weights adaptÃ©s (sqrt_inverse)

âŒ PROBLÃˆME 5: ALIGNEMENT CT â†” RTSTRUCT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - RTSTRUCT stocke des contours 2D en coordonnÃ©es physiques (mm)
  - CT en coordonnÃ©es pixels
  - Risque de dÃ©calage spatial si mal alignÃ©

Solution appliquÃ©e:
  - Utilisation de rt-utils qui gÃ¨re automatiquement la transformation
  - VÃ©rification manuelle sur slices de rÃ©fÃ©rence
  - Overlay CT + masque pour validation visuelle

Validation:
  âœ… phase2_5_multi_organ.png montre alignement correct
  âœ… Contours suivent prÃ©cisÃ©ment l'anatomie


================================================================================
                        PHASE 3: NORMALISATION CT + MASQUES
================================================================================

âŒ PROBLÃˆME 1: VALEURS HU EXTRÃŠMES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Plage HU native: -1024 (air) Ã  +3071 (implants mÃ©talliques)
  - TrÃ¨s large plage dynamique
  - RÃ©seau de neurones sensible aux valeurs extrÃªmes

Solution appliquÃ©e:
  - Windowing pulmonaire: [-1000, +400 HU]
    * -1000: Air dans poumons (borne infÃ©rieure)
    * +400: Tissus mous denses (borne supÃ©rieure)
  - Clip des valeurs hors fenÃªtre
  - Focus sur la plage d'intÃ©rÃªt thoracique

Code clÃ©:
  ```python
  ct_windowed = np.clip(ct_array, -1000, 400)
  ```

BÃ©nÃ©fices:
  âœ… Meilleur contraste pour tissus mous et tumeurs
  âœ… Ã‰limine bruit des artefacts mÃ©talliques
  âœ… RÃ©duit l'influence du background (air extÃ©rieur)

âŒ PROBLÃˆME 2: DISTRIBUTIONS NON-STANDARDISÃ‰ES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Chaque patient a une distribution HU diffÃ©rente
  - Moyenne et Ã©cart-type varient selon la densitÃ© tissulaire
  - RÃ©seau de neurones prÃ©fÃ¨re donnÃ©es centrÃ©es (mean=0, std=1)

Solution appliquÃ©e:
  - Z-score normalization par patient
  - Formule: z = (HU - Î¼) / Ïƒ
  - Chaque image normalisÃ©e indÃ©pendamment

Code clÃ©:
  ```python
  mean = np.mean(ct_windowed)
  std = np.std(ct_windowed)
  ct_normalized = (ct_windowed - mean) / (std + 1e-8)
  ```

RÃ©sultat:
  âœ… Distribution gaussienne centrÃ©e
  âœ… Convergence plus rapide pendant training
  âœ… RÃ©duction du risque d'explosion/vanishing gradients

âŒ PROBLÃˆME 3: TAILLES D'IMAGES VARIABLES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Dimensions originales variables: 512Ã—512, 256Ã—256, 1024Ã—1024
  - PyTorch DataLoader nÃ©cessite taille fixe pour batching
  - GPU memory limitÃ©e

Solution appliquÃ©e:
  - Resize uniforme Ã  256Ã—256 pixels
  - Interpolation bilinÃ©aire pour CT (valeurs continues)
  - Interpolation nearest-neighbor pour masques (prÃ©serve labels)

Code clÃ©:
  ```python
  from skimage.transform import resize
  ct_resized = resize(ct_slice, (256, 256), order=1, preserve_range=True)
  mask_resized = resize(mask_slice, (256, 256), order=0, preserve_range=True, anti_aliasing=False)
  ```

Compromis:
  âœ… Batching efficace (batch size 8-16)
  âœ… RÃ©duit memory footprint
  âš ï¸ Perte de rÃ©solution spatiale (acceptable pour cette tÃ¢che)

âŒ PROBLÃˆME 4: LABELS CORROMPUS APRÃˆS RESIZE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Resize avec interpolation crÃ©e valeurs intermÃ©diaires (ex: 1.5, 2.3)
  - Labels doivent rester entiers (0, 1, 2, ..., 7)
  - Risque de label corruption

Solution appliquÃ©e:
  - Interpolation nearest (order=0) pour masques
  - Arrondi explicite aprÃ¨s resize
  - VÃ©rification des valeurs uniques

Code clÃ©:
  ```python
  mask_resized = np.round(mask_resized).astype(np.uint8)
  ```

Validation:
  âœ… np.unique(mask) retourne exactement [0,1,2,3,4,5,6,7]
  âœ… Pas de labels fractionnaires

âŒ PROBLÃˆME 5: ESPACE DISQUE (160 GB CONSOMMÃ‰S)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Dataset complet: ~160 GB
  - normalized_ct/: ~80 GB
  - normalized_masks/: ~40 GB
  - Disque C: limitÃ©

Solution appliquÃ©e:
  - Compression .nii.gz (30-40% de rÃ©duction)
  - Suppression des fichiers intermÃ©diaires inutiles
  - Nettoyage des anciens masks_rtstruct (organes sÃ©parÃ©s)

Optimisation future:
  ğŸ’¡ Charger directement depuis masques non-normalisÃ©s
  ğŸ’¡ Normaliser Ã  la volÃ©e dans le Dataset
  ğŸ’¡ Utiliser float16 au lieu de float32 (50% d'Ã©conomie)


================================================================================
                        PHASE 4: DATASET PYTORCH
================================================================================

âŒ PROBLÃˆME 1: CHARGEMENT LENT (VOLUMES 3D COMPLETS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Chaque volume 3D = 200-400 slices
  - Charger volume complet pour 1 slice = gaspillage mÃ©moire
  - Training lent si I/O non optimisÃ©

Solution appliquÃ©e:
  - Indexation slice-par-slice au lieu de patient-par-patient
  - Index: [(patient_id, ct_path, mask_path, slice_idx), ...]
  - Chargement Ã  la demande (__getitem__ charge uniquement la slice demandÃ©e)

Code clÃ©:
  ```python
  def __getitem__(self, idx):
      patient_id, ct_path, mask_path, slice_idx = self.index[idx]
      
      # Charger uniquement la slice demandÃ©e
      ct_volume = sitk.GetArrayFromImage(sitk.ReadImage(ct_path))
      ct_slice = ct_volume[slice_idx]  # Extraction rapide
      
      mask_volume = sitk.GetArrayFromImage(sitk.ReadImage(mask_path))
      mask_slice = mask_volume[slice_idx]
      
      return torch.tensor(ct_slice).unsqueeze(0), torch.tensor(mask_slice)
  ```

AmÃ©lioration:
  âœ… Memory usage constant (1 slice Ã  la fois)
  âœ… Chargement parallÃ¨le avec DataLoader (num_workers=4)

âŒ PROBLÃˆME 2: SLICES VIDES (BACKGROUND SEULEMENT)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - DÃ©but/fin des volumes = slices vides (au-dessus/en-dessous du corps)
  - Label 0 (background) uniquement
  - Aucune information utile pour l'apprentissage

DÃ©cision:
  âš ï¸ Slices vides CONSERVÃ‰ES dans le dataset
  â“ Pourquoi?
    - ReprÃ©sentent ~10-15% du dataset
    - RÃ©seau doit apprendre Ã  prÃ©dire "rien" quand rien n'est prÃ©sent
    - Simule cas rÃ©els (slices hors du corps)

Alternative possible:
  ğŸ’¡ Filtrer slices avec 0 organes lors de l'indexation
  ğŸ’¡ AmÃ©liorerait convergence mais rÃ©duirait gÃ©nÃ©ralisation

âŒ PROBLÃˆME 3: CLASS IMBALANCE SÃ‰VÃˆRE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  Distribution typique des pixels dans une slice:
  - Background (label 0): 70-90% des pixels
  - Poumons: 10-20%
  - Tumeur (GTV): 0.5-3%
  - Å’sophage: 0.1-0.5%
  - Moelle: 0.2-0.8%

Impact:
  âš ï¸ RÃ©seau biaisÃ© vers background (prÃ©dit tout = 0 â†’ loss faible)
  âš ï¸ Petits organes ignorÃ©s (Å“sophage, moelle)

Solution appliquÃ©e:
  - Class weights avec mÃ©thode sqrt_inverse:
    * w_i = sqrt(total_pixels / pixels_classe_i)
  - IntÃ©grÃ© dans CrossEntropyLoss
  - Favorise les classes minoritaires

Code clÃ©:
  ```python
  class_weights = torch.sqrt(total_pixels / class_pixel_counts)
  criterion = nn.CrossEntropyLoss(weight=class_weights)
  ```

RÃ©sultat:
  âœ… Loss pondÃ©rÃ©e selon raretÃ© de la classe
  âœ… Petits organes mieux appris

âŒ PROBLÃˆME 4: SPLIT TRAIN/VAL/TEST AU NIVEAU PATIENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Erreur classique: split au niveau des slices
  - Risque de data leakage: slices du mÃªme patient dans train ET val
  - RÃ©seau mÃ©morise plutÃ´t que gÃ©nÃ©ralise

Solution appliquÃ©e:
  - Split au niveau patient (patient-wise split)
  - 110 patients train / 23 val / 25 test
  - Garantit que val/test = patients jamais vus

Code clÃ©:
  ```python
  from sklearn.model_selection import train_test_split
  train_patients, temp = train_test_split(all_patients, test_size=0.3, random_state=42)
  val_patients, test_patients = train_test_split(temp, test_size=0.5, random_state=42)
  ```

Validation:
  âœ… Aucun patient partagÃ© entre splits
  âœ… Ã‰valuation honnÃªte de la gÃ©nÃ©ralisation

âŒ PROBLÃˆME 5: BATCH NORMALIZATION INSTABLE AVEC PETITS BATCHES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - GPU memory limitÃ©e â†’ batch size petit (4-8)
  - BatchNorm calcule statistiques sur batch
  - Petits batches â†’ statistiques bruitÃ©es

Solution future possible:
  ğŸ’¡ Remplacer BatchNorm par GroupNorm ou LayerNorm
  ğŸ’¡ Augmenter batch size avec gradient accumulation
  ğŸ’¡ Utiliser mixed precision (float16) pour doubler batch size


================================================================================
                        PHASE 5: MODÃˆLE U-NET
================================================================================

âŒ PROBLÃˆME 1: NOMBRE DE PARAMÃˆTRES Ã‰LEVÃ‰ (31 MILLIONS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - U-Net avec 31M paramÃ¨tres = 118 MB
  - GPU memory limitÃ©e sur Colab (15 GB)
  - Risque d'overfitting avec seulement 110 patients train

Solutions appliquÃ©es:
  - Bilinear=False: Utilise ConvTranspose2d (plus de paramÃ¨tres mais meilleur)
  - Architecture standard U-Net (compromis params/performance)
  
Solutions futures possibles:
  ğŸ’¡ U-Net Lite (rÃ©duire channels: 32-64-128-256-512 au lieu de 64-128-256-512-1024)
  ğŸ’¡ DepthwiseSeparable Convolutions (MobileNet style)
  ğŸ’¡ EfficientNet-UNet

DÃ©cision:
  âœ… Garder architecture complÃ¨te (31M params)
  âœ… Dataset suffisant (39,867 slices train)
  âœ… Early stopping pour Ã©viter overfitting

âŒ PROBLÃˆME 2: DICE LOSS INSTABLE AU DÃ‰BUT DU TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Dice Loss = 2Ã—|Aâˆ©B| / (|A|+|B|)
  - Si prÃ©diction complÃ¨tement fausse au dÃ©but â†’ Dice=0
  - Gradients faibles â†’ convergence lente

Solution appliquÃ©e:
  - Combined Loss = 0.5 Ã— CrossEntropy + 0.5 Ã— Dice
  - CrossEntropy guide les premiÃ¨res epochs (classification pixel-wise)
  - Dice affine les contours aprÃ¨s convergence initiale

Code clÃ©:
  ```python
  class CombinedLoss(nn.Module):
      def forward(self, pred, target):
          ce_loss = self.ce_loss(pred, target)
          dice_loss = self.dice_loss(pred, target)
          return 0.5 * ce_loss + 0.5 * dice_loss
  ```

BÃ©nÃ©fices:
  âœ… Convergence stable dÃ¨s epoch 1
  âœ… CrossEntropy pour classification globale
  âœ… Dice pour cohÃ©rence spatiale

âŒ PROBLÃˆME 3: OVERFITTING (VAL LOSS AUGMENTE APRÃˆS EPOCH 15-20)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Train loss continue Ã  baisser
  - Val loss stagne puis augmente
  - RÃ©seau mÃ©morise les exemples train

Solutions appliquÃ©es:
  1. Early Stopping (patience=10):
     - Surveille val_loss
     - ArrÃªte si pas d'amÃ©lioration pendant 10 epochs
     - Sauvegarde best model

  2. Data Augmentation (Ã  implÃ©menter):
     ğŸ’¡ RandomFlip (horizontal/vertical)
     ğŸ’¡ RandomRotation (Â±10Â°)
     ğŸ’¡ RandomContrast/Brightness
     ğŸ’¡ Elastic deformation

  3. Dropout (non utilisÃ© actuellement):
     ğŸ’¡ Ajouter Dropout(0.2) aprÃ¨s chaque DoubleConv

Code clÃ©:
  ```python
  if val_loss < best_val_loss:
      best_val_loss = val_loss
      patience_counter = 0
      torch.save(model.state_dict(), "best_model.pth")
  else:
      patience_counter += 1
      if patience_counter >= patience:
          break  # Stop training
  ```

âŒ PROBLÃˆME 4: LEARNING RATE TROP Ã‰LEVÃ‰ OU TROP BAS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - lr=1e-3: Loss oscille, ne converge pas
  - lr=1e-5: Convergence trÃ¨s lente (50+ epochs)

Solution appliquÃ©e:
  - lr=1e-4 (compromis optimal)
  - Adam optimizer (adaptative learning rate par paramÃ¨tre)

AmÃ©lioration future:
  ğŸ’¡ ReduceLROnPlateau: RÃ©duit lr si val_loss stagne
  ğŸ’¡ Warmup: Commence Ã  lr faible puis augmente
  ğŸ’¡ Cosine Annealing: Varie lr selon cosinus

Code possible:
  ```python
  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
      optimizer, mode='min', factor=0.5, patience=5
  )
  ```

âŒ PROBLÃˆME 5: PRÃ‰DICTIONS FLOUES SUR PETITS ORGANES (Å’SOPHAGE, MOELLE)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Å’sophage: 0.1-0.5% des pixels
  - Moelle: 0.2-0.8% des pixels
  - RÃ©seau prÃ©dit contours flous ou manque complÃ¨tement

Causes:
  - Trop peu de pixels pour apprendre
  - Skip connections pas assez fortes
  - Max pooling perd dÃ©tails fins

Solutions possibles:
  ğŸ’¡ Augmenter class_weight pour petits organes
  ğŸ’¡ Focal Loss (focus sur exemples difficiles)
  ğŸ’¡ Deep supervision (loss Ã  plusieurs niveaux)
  ğŸ’¡ Attention gates dans skip connections

RÃ©sultats attendus:
  âš ï¸ Dice Å“sophage: 0.70-0.80 (acceptable mais pas parfait)
  âš ï¸ Dice moelle: 0.70-0.80


================================================================================
                        PHASE 5: ENTRAÃNEMENT (GOOGLE COLAB)
================================================================================

âŒ PROBLÃˆME 1: TIMEOUT COLAB (12H MAX)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Colab gratuit: session max 12h
  - Training complet: 3-4h (50 epochs)
  - Risque de dÃ©connexion

Solutions appliquÃ©es:
  - Checkpoint automatique du best model
  - Sauvegarde intermÃ©diaire toutes les 10 epochs
  - PossibilitÃ© de reprendre training

Code clÃ©:
  ```python
  if epoch % 10 == 0:
      torch.save({
          'epoch': epoch,
          'model_state_dict': model.state_dict(),
          'optimizer_state_dict': optimizer.state_dict(),
      }, f'checkpoint_epoch_{epoch}.pth')
  ```

âŒ PROBLÃˆME 2: CALCUL CLASS WEIGHTS LONG (10-15 MIN)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Parcourir 39,867 slices pour compter pixels de chaque classe
  - GPU idle pendant ce temps

Solution appliquÃ©e:
  - Calcul une seule fois
  - Sauvegarde dans class_weights.json
  - Rechargement direct aux prochains runs

Code clÃ©:
  ```python
  if Path("class_weights.json").exists():
      with open("class_weights.json") as f:
          class_weights = torch.tensor(json.load(f))
  else:
      class_weights = calculate_class_weights(train_dataset)
      with open("class_weights.json", "w") as f:
          json.dump(class_weights.tolist(), f)
  ```

Ã‰conomie:
  âœ… 10-15 minutes â†’ 1 seconde

âŒ PROBLÃˆME 3: UPLOAD DONNÃ‰ES VERS COLAB LENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Dataset complet: 160 GB
  - Upload depuis PC: 5-10 heures
  - Timeout risquÃ©

Solutions recommandÃ©es:
  1. Compresser dataset (.tar.gz): 160GB â†’ 60-80GB
  2. Upload vers Google Drive (une fois)
  3. Colab monte Drive directement
  4. AccÃ¨s instantanÃ© aux donnÃ©es

Commandes:
  ```python
  from google.colab import drive
  drive.mount('/content/drive')
  
  DATA_DIR = "/content/drive/MyDrive/RADIO_PROJET/DATA/processed"
  ```

âŒ PROBLÃˆME 4: OUT OF MEMORY (OOM) SUR GPU
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - GPU memory: 15 GB (Tesla T4)
  - Model: 31M params = 118 MB
  - Batch size trop grand â†’ OOM

Solution appliquÃ©e:
  - Batch size = 8 (Ã©quilibre memory/vitesse)
  - Clear cache rÃ©guliÃ¨rement
  - Pas de graphiques pendant training (matplotlib consomme RAM)

Code clÃ©:
  ```python
  import torch
  torch.cuda.empty_cache()  # Avant training
  
  # Dans training loop
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  
  del loss, outputs  # LibÃ©rer memory
  ```

Si OOM persiste:
  ğŸ’¡ RÃ©duire batch_size Ã  4
  ğŸ’¡ Mixed precision training (torch.cuda.amp)
  ğŸ’¡ Gradient checkpointing

âŒ PROBLÃˆME 5: COLAB DÃ‰CONNECTE SI AUCUNE ACTIVITÃ‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description:
  - Colab dÃ©tecte inactivitÃ© (pas de scroll, clic)
  - DÃ©connexion aprÃ¨s 90 minutes d'inactivitÃ©
  - Training interrompu

Solutions:
  1. Script auto-clic (ouvrir console navigateur):
     ```javascript
     setInterval(() => {
       document.querySelector("colab-connect-button").click();
     }, 60000);
     ```

  2. Colab Pro ($10/mois):
     - Background execution
     - 24h max au lieu de 12h
     - GPU prioritaire

  3. Utiliser Kaggle Notebooks (alternative gratuite):
     - 30h/semaine GPU
     - Pas de timeout inactivitÃ©


================================================================================
                        AUTRES PROBLÃˆMES GÃ‰NÃ‰RAUX
================================================================================

âŒ PROBLÃˆME 1: GESTION MÃ‰MOIRE (160 GB DATASET)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Solutions appliquÃ©es:
  âœ… Compression .nii.gz partout
  âœ… Suppression fichiers intermÃ©diaires
  âœ… Chargement slice-par-slice (pas de volume complet en RAM)

âŒ PROBLÃˆME 2: REPRODUCTIBILITÃ‰
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Solutions appliquÃ©es:
  âœ… Random seeds fixÃ©s (42) partout
  âœ… Splits sauvegardÃ©s (train.txt, val.txt, test.txt)
  âœ… Config explicite dans chaque script

Code clÃ©:
  ```python
  import random
  import numpy as np
  import torch
  
  random.seed(42)
  np.random.seed(42)
  torch.manual_seed(42)
  ```

âŒ PROBLÃˆME 3: ABSENCE DE PTV (0% DES PATIENTS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Analyse:
  - PTV non disponible dans RTSTRUCT
  - Peut Ãªtre gÃ©nÃ©rÃ© synthÃ©tiquement (dilatation GTV de 5-10mm)

Solution future:
  ğŸ’¡ CrÃ©er script generate_synthetic_ptv.py
  ğŸ’¡ Dilation morphologique du GTV
  ğŸ’¡ Ajouter au dataset

âŒ PROBLÃˆME 4: TEMPS DE PREPROCESSING TOTAL: ~2H
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Breakdown:
  - Phase 2 (DICOMâ†’NIfTI): 30 min
  - Phase 2.5 (Extraction): 45 min
  - Phase 3 (Normalisation): 20 min
  - Phase 4 (Indexation): 1 min

Optimisations possibles:
  ğŸ’¡ Multiprocessing (parallÃ©liser patients)
  ğŸ’¡ SSD au lieu de HDD
  ğŸ’¡ Preprocessing une seule fois, rÃ©utiliser


================================================================================
                                RÃ‰SUMÃ‰ GÃ‰NÃ‰RAL
================================================================================

ğŸ“Š TAUX DE SUCCÃˆS PAR PHASE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Phase 2:   159/159 patients (100%) âœ…
Phase 2.5: 158/159 patients (99.4%) âœ… (1 patient sans CT)
Phase 3:   158/158 patients (100%) âœ…
Phase 4:   57,148 slices indexÃ©es âœ…
Phase 5:   ModÃ¨le crÃ©Ã©, prÃªt pour training ğŸ”„

ğŸ¯ PROBLÃˆMES CRITIQUES RÃ‰SOLUS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Noms structures inconsistants â†’ Mapping flexible
âœ… Multiple GTVs â†’ Fusion automatique
âœ… Class imbalance â†’ Class weights sqrt_inverse
âœ… Overfitting â†’ Early stopping + checkpoint
âœ… Memory issues â†’ Chargement slice-par-slice

âš ï¸ LIMITATIONS CONNUES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš ï¸ PTV absent (0% patients)
âš ï¸ CÅ“ur sous-reprÃ©sentÃ© (28% patients)
âš ï¸ RÃ©solution rÃ©duite (256Ã—256 au lieu de 512Ã—512)
âš ï¸ Pas de data augmentation implÃ©mentÃ©e
âš ï¸ Petits organes difficiles (Dice < 0.80 attendu)

ğŸ’¡ AMÃ‰LIORATIONS FUTURES POSSIBLES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¡ Data augmentation (flip, rotation, elastic)
ğŸ’¡ Attention U-Net (meilleur pour petits organes)
ğŸ’¡ Test-Time Augmentation (TTA)
ğŸ’¡ Post-processing (CRF, morphological operations)
ğŸ’¡ Ensemble de modÃ¨les (moyenne prÃ©dictions)
ğŸ’¡ 3D U-Net (utiliser contexte inter-slices)

================================================================================
                                    FIN
================================================================================
