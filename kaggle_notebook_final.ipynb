{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53f2d2f",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# âš ï¸ KAGGLE SETUP - Choose your option:\n",
    "\n",
    "# OPTION A: Dataset uploaded to Kaggle (RECOMMENDED)\n",
    "# After uploading to Kaggle Datasets, use this:\n",
    "# DATA_ROOT = Path('/kaggle/input/nsclc-multiorgan-segmentation')\n",
    "\n",
    "# OPTION B: Dataset on external source (slower)\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  ğŸ“Œ SETUP INSTRUCTIONS FOR KAGGLE                             â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  1. Upload kaggle_dataset.zip to Kaggle Datasets              â•‘\n",
    "â•‘  2. In this notebook: Click \"+ Add Data\" (top right)          â•‘\n",
    "â•‘  3. Search for your dataset name                              â•‘\n",
    "â•‘  4. Add it to this notebook                                    â•‘\n",
    "â•‘  5. Update DATA_ROOT below with correct path                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Update this path after adding your dataset\n",
    "DATA_ROOT = Path('/kaggle/input/nsclc-multiorgan-segmentation/nsclc_multiorgan_segmentation')\n",
    "\n",
    "print(f\"Dataset path: {DATA_ROOT}\")\n",
    "print(\"âš ï¸ Make sure to update DATA_ROOT with your actual dataset path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0a78f",
   "metadata": {},
   "source": [
    "## ğŸ”§ Cell 2: Setup Paths & Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37436e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Update this after adding your dataset to Kaggle\n",
    "DATA_ROOT = Path('/kaggle/input/YOUR-DATASET-NAME-HERE/nsclc_multiorgan_segmentation')\n",
    "\n",
    "# Check if path exists\n",
    "if not DATA_ROOT.exists():\n",
    "    print(\"âŒ Dataset not found!\")\n",
    "    print(f\"âŒ Path: {DATA_ROOT}\")\n",
    "    print(\"\\nğŸ“ To fix:\")\n",
    "    print(\"  1. Click '+ Add Data' (top right)\")\n",
    "    print(\"  2. Search for your uploaded dataset\")\n",
    "    print(\"  3. Add it to this notebook\")\n",
    "    print(\"  4. Update DATA_ROOT above with correct path\")\n",
    "else:\n",
    "    print(\"âœ… Dataset found!\")\n",
    "    print(f\"ğŸ“‚ Path: {DATA_ROOT}\")\n",
    "    \n",
    "    # Check structure\n",
    "    ct_files = list((DATA_ROOT / 'normalized_ct').glob('*.nii.gz'))\n",
    "    mask_files = list((DATA_ROOT / 'normalized_masks').glob('*.nii.gz'))\n",
    "    code_files = list((DATA_ROOT / 'code').glob('*.py'))\n",
    "    \n",
    "    print(f\"âœ… CT scans: {len(ct_files)} files\")\n",
    "    print(f\"âœ… Masks: {len(mask_files)} files\")\n",
    "    print(f\"âœ… Code files: {code_files}\")\n",
    "    \n",
    "    # Add code to path\n",
    "    sys.path.append(str(DATA_ROOT / 'code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5992c",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Cell 3: Install Dependencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SimpleITK if needed\n",
    "!pip install SimpleITK -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# Import custom modules\n",
    "from dataset_multi_organ import MultiOrganDataset\n",
    "from unet_multi_organ import UNetMultiOrgan\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877c081",
   "metadata": {},
   "source": [
    "## âš™ï¸ Cell 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_workers': 2,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'patience': 10,\n",
    "    'in_channels': 1,\n",
    "    'out_channels': 8,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ee7be",
   "metadata": {},
   "source": [
    "## ğŸ“Š Cell 5: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_dataset = MultiOrganDataset(\n",
    "    data_root=str(DATA_ROOT),\n",
    "    split='train',\n",
    "    slice_wise=True\n",
    ")\n",
    "\n",
    "val_dataset = MultiOrganDataset(\n",
    "    data_root=str(DATA_ROOT),\n",
    "    split='val',\n",
    "    slice_wise=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train: {len(train_dataset)} slices\")\n",
    "print(f\"âœ… Val: {len(val_dataset)} slices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b8de0",
   "metadata": {},
   "source": [
    "## ğŸ§  Cell 6: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetMultiOrgan(\n",
    "    in_channels=1,\n",
    "    out_channels=8,\n",
    "    bilinear=False\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ… Model: {num_params:,} parameters\")\n",
    "print(f\"âœ… Model size: {num_params * 4 / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726558a",
   "metadata": {},
   "source": [
    "## ğŸ¯ Cell 7: Loss Functions & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        target_one_hot = torch.nn.functional.one_hot(target, 8).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return 0.5 * self.ce(pred, target) + 0.5 * self.dice(pred, target)\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "print(\"âœ… Loss: CombinedLoss (CE + Dice)\")\n",
    "print(\"âœ… Optimizer: Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35d90e",
   "metadata": {},
   "source": [
    "## ğŸ”„ Cell 8: Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"âœ… Training functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4d5cc",
   "metadata": {},
   "source": [
    "## ğŸš€ Cell 9: Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\nğŸ“Š Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, CONFIG['device'])\n",
    "    val_loss = validate(model, val_loader, criterion, CONFIG['device'])\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/content/best_model.pth')\n",
    "        print(\"âœ… Saved best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONFIG['patience']:\n",
    "            print(f\"âš ï¸ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… TRAINING COMPLETED\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d87b6",
   "metadata": {},
   "source": [
    "## ğŸ“Š Cell 10: Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('/content/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ“Š Best Val Loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"ğŸ“Š Final Train Loss: {history['train_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007eace",
   "metadata": {},
   "source": [
    "## ğŸ¨ Cell 11: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger best model\n",
    "model.load_state_dict(torch.load('/content/best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Prendre 4 samples du validation set\n",
    "val_samples = [val_dataset[i] for i in range(0, len(val_dataset), len(val_dataset)//4)][:4]\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(val_samples):\n",
    "        image = sample['image'].unsqueeze(0).to(CONFIG['device'])\n",
    "        mask_true = sample['mask'].numpy()\n",
    "        \n",
    "        # PrÃ©diction\n",
    "        output = model(image)\n",
    "        mask_pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx, 0].imshow(image.cpu().squeeze(), cmap='gray')\n",
    "        axes[idx, 0].set_title('CT Scan')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        axes[idx, 1].imshow(mask_true, cmap='tab10', vmin=0, vmax=7)\n",
    "        axes[idx, 1].set_title('Ground Truth')\n",
    "        axes[idx, 1].axis('off')\n",
    "        \n",
    "        axes[idx, 2].imshow(mask_pred, cmap='tab10', vmin=0, vmax=7)\n",
    "        axes[idx, 2].set_title('Prediction')\n",
    "        axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0257624",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Cell 12: Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ddc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copier les rÃ©sultats vers Drive\n",
    "!cp /content/best_model.pth \"/content/drive/MyDrive/NSCLC_MultiOrgan_Dataset/\"\n",
    "!cp /content/training_curves.png \"/content/drive/MyDrive/NSCLC_MultiOrgan_Dataset/\"\n",
    "!cp /content/predictions.png \"/content/drive/MyDrive/NSCLC_MultiOrgan_Dataset/\"\n",
    "\n",
    "print(\"âœ… Results saved to Google Drive!\")\n",
    "print(\"ğŸ“‚ Location: MyDrive/NSCLC_MultiOrgan_Dataset/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
